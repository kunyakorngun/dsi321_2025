from prefect import flow, task
from playwright.sync_api import sync_playwright
from prefect.schedules import Interval
from datetime import timedelta
from pathlib import Path
import pandas as pd
from datetime import datetime
import time
import logging
from config_path import keywords

logger = logging.getLogger("scrape")
logger.setLevel('INFO')
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)

formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(name)s | %(message)s')
console_handler.setFormatter(formatter)

logger.addHandler(console_handler)

@task(name="Search News")
def search_news(keyword: str, max_pages: int = 1):#‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô1‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏à‡πÑ‡∏î‡πâ‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡πÇ‡∏î‡∏¢‡∏à‡∏≤‡∏Å‡∏•‡∏¥‡πâ‡∏áurl‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ‡∏°‡∏±‡∏ô‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏Ø‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        search_url = f"https://www.google.com/search?q={keyword.replace(' ', '+')}&tbm=nws&tbs=sbd:1"
        page.goto(search_url)

        titles_links = []
        seen_titles = set()

        for page_num in range(1, max_pages + 1):
            logger.info(f"üìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà {page_num}")
            time.sleep(2)
            page.keyboard.press("End")
            time.sleep(2)

            titles = page.query_selector_all('div[role="heading"][aria-level="3"]')
            for title_el in titles:
                try:
                    title = title_el.text_content().strip()
                    a_tag = title_el.query_selector("xpath=ancestor::a")
                    link = a_tag.get_attribute("href") if a_tag else None
                    if title and link and title not in seen_titles:
                        titles_links.append((title, link))
                        seen_titles.add(title)
                except Exception as e:
                    logger.error(f"‚ùå Error ‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ {page_num}: {e}")
                    continue

            next_button = page.query_selector('#pnnext')
            if next_button:
                next_button.click()
            else:
                break

        browser.close()
        return titles_links

@task(name="save to csv")
def save_to_csv(data, filename="news_results.csv"):
    df = pd.DataFrame(data)
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    logger.info(f"‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏£‡∏ß‡∏°‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(df)} ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô {filename}")

@flow(name="incremental flow")
def news_scraper_flow():
    all_results = []

    for keyword in keywords:
        results = search_news(keyword)
        for title, link in results:
            all_results.append({
                "Fetched Time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "Keyword": keyword,
                "Title": title,
                "Link": link
            })

    save_to_csv(all_results)

if __name__ == "__main__":
    news_scraper_flow.from_source(
        source=Path(__file__).parent,
        entrypoint="./scrape_2.py:news_scraper_flow",
    ).deploy(
        name="incremental flow",
        work_pool_name="scrape-news",
        schedule=Interval(
            timedelta(minutes=2),
            timezone="Asia/Bangkok"
        )
    )
    # print(Path(__file__).parent)